{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n",
    "<br><br>\n",
    "<center><u><H1>Part of Speech Tagging</H1></u></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><H3> Requirement: JAVA jdk installation and JAVA_HOME variable</H3>\n",
    "    \n",
    "1) Download JAVA jdk from :\n",
    "    https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html \n",
    "    \n",
    "2) Install in: C:\\Program Files\\jre1.8.0_181    \n",
    "    \n",
    "3) Create a Java Home environment variable consigning the route of jdk file\n",
    "\n",
    "4) Create a Java Home variable\n",
    "\n",
    "    System Variable:\n",
    "\n",
    "    Variable name: JAVA_HOME\n",
    "    Variable value: C:\\Program Files\\Java\\jdk1.8.0_181\n",
    "\n",
    "5) Update System Path:\n",
    "\n",
    "    -Edit\n",
    "\n",
    "    -New\n",
    "\n",
    "    -%JAVA_HOME%\\bin\n",
    "\n",
    "6) Test your installation:\n",
    "\n",
    "    Open cmd \n",
    "    echo %JAVA_HOME%\n",
    "\n",
    "    javac -version\n",
    "\n",
    "7) Reboot your system</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"I was watching movies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('watching', 'VBG'), ('movies', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE TAGES FROM THE 36 PENN TREE TAGGERS BY PENSYLAVANIA UNIVERSITY\n",
    "# PRP: Personal pronoun\n",
    "# VBD: Verb, past tense\n",
    "# VBG: Veb, gerund\n",
    "# NNS: Noun plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving all nouns\n",
    "s = 'My favourite scientist is Carl Sagan'\n",
    "tagged = pos_tag(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scientist', 'Carl', 'Sagan']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering only NN and NNP words form the sentence\n",
    "allnoun = [word for word, pos in tagged if pos in ['NN','NNP']]\n",
    "allnoun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('life', 'NN'), ('is', 'VBZ'), ('beautiful', 'JJ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jar = '../Resources/stanford-postagger/stanford-postagger.jar'\n",
    "model = '../Resources/stanford-postagger/models/english-bidirectional-distsim.tagger'\n",
    "pos_tagger = StanfordPOSTagger(model, jar)\n",
    "pos_tagger.tag('The life is beautiful'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 'WP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('airspeed', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('unladen', 'JJ'),\n",
       " ('swallow', 'VB'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = StanfordPOSTagger('../Resources/stanford-postagger-full/models/english-bidirectional-distsim.tagger','../Resources/stanford-postagger-full/stanford-postagger.jar')\n",
    "st.tag('What is the airspeed of an unladen swallow ?'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Sai Charan\n",
      "[nltk_data]     Reddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VBD', 2524),\n",
       " ('CC', 2664),\n",
       " ('JJ', 4392),\n",
       " ('.', 4452),\n",
       " ('NNS', 5066),\n",
       " (',', 5133),\n",
       " ('NP', 6866),\n",
       " ('AT', 8893),\n",
       " ('IN', 10616),\n",
       " ('NN', 13162)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "freq = nltk.FreqDist(tags)\n",
    "tags_freq = sorted(freq.items(), key=operator.itemgetter(1))\n",
    "tags_freq[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13089484257215028\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "print(default_tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Tagger:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram tagger takes previous n words in the context, to predict the POS tag for the given token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets # train = 90%(0.9) and test 10%((0.9):) data ()\n",
    "train_data = brown_tagged_sents[:int(len(brown_tagged_sents) * 0.9)]\n",
    "test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361407355726104\n"
     ]
    }
   ],
   "source": [
    "#unigram considers the conditional frequency of tags and predicts the most\n",
    "#frequent tag for the every given token.\n",
    "unigram_tagger = UnigramTagger(train_data, backoff=default_tagger)\n",
    "print(unigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8452108043456593\n"
     ]
    }
   ],
   "source": [
    "#bigram consider the tags of the given word and previous word, and tag as\n",
    "#tuple to get the given tag for the test word.\n",
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print(bigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843317053722715\n"
     ]
    }
   ],
   "source": [
    "#trigram looks for the previous two words with the similar process.\n",
    "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "print(trigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are combining the three taggers. First it will look for the Trigram\n",
    "#of the given word sequence for predicting the tag, if not found it Backoff \n",
    "#to BigramTagger parameter and to a UnigramTagger and in the end to a NN tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tagger = RegexpTagger(\n",
    "[(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), #cardinal numbers\n",
    " (r'(The|the|A|a|An|an)$', 'AT'), #articles\n",
    " (r'.*able$', 'JJ'), #adjectives\n",
    " (r'.*ness$', 'NN'), #nouns formed from adj\n",
    " (r'.*ly$', 'RB'), #adverbs\n",
    " (r'.*s$', 'NNS'), #plural nouns\n",
    " (r'.*ing$', 'VRG'), #gerunds\n",
    " (r'.*ed$', 'VBD'), #past tense verbs\n",
    " (r'.*', 'NN') # nouns (default)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2999102960231237\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ne_chunk method recognizes people(names), places(location),\n",
    "#and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Stephen Hawking teach maths at the Oxford University in England\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\map25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Stephen/NNP)\n",
      "  Hawking/NNP\n",
      "  teach/VB\n",
      "  maths/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Oxford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE England/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(nltk.pos_tag(word_tokenize(text)), binary=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\map25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "(S\n",
      "  (NE Stephen/NNP)\n",
      "  Hawking/NNP\n",
      "  teach/VB\n",
      "  maths/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE Oxford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (NE England/NNP))\n"
     ]
    }
   ],
   "source": [
    "# if bynary parameter is True it provides the output for the entire\n",
    "# sentence tree and tags everything.\n",
    "print(ne_chunk(nltk.pos_tag(word_tokenize(text)), binary=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carl', 'PERSON'),\n",
       " ('Sagan', 'PERSON'),\n",
       " ('taught', 'O'),\n",
       " ('at', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Cornell', 'ORGANIZATION'),\n",
       " ('University', 'ORGANIZATION'),\n",
       " ('in', 'O'),\n",
       " ('USA', 'LOCATION')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jar_ner = '../Resources/stanford-ner/stanford-ner.jar'\n",
    "model_ner = '../Resources/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "st_ner = StanfordNERTagger(model_ner, jar_ner)\n",
    "st_ner.tag('Carl Sagan taught at the Cornell University in USA'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "http://www.nltk.org/book/ch02.html\n",
    "\n",
    "https://nlp.stanford.edu/software/\n",
    "\n",
    "http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\n",
    "\n",
    "https://en.wikipedia.org/wiki/Brown_Corpus\n",
    "\n",
    "https://nlp.stanford.edu/software/CRF-NER.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

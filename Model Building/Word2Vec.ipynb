{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><u><H1>Word2Vec</H1></u></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Sai Charan\n",
      "[nltk_data]     Reddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package treebank to C:\\Users\\Sai Charan\n",
      "[nltk_data]     Reddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    }
   ],
   "source": [
    "# Brown, movie reviews and treebank are text data\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('treebank')\n",
    "br = Word2Vec(brown.sents())\n",
    "mr = Word2Vec(movie_reviews.sents())\n",
    "tb = Word2Vec(treebank.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Charan Reddy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('care', 0.9181945323944092),\n",
       " ('job', 0.9136676788330078),\n",
       " ('trouble', 0.9118092656135559),\n",
       " ('chance', 0.8856303691864014),\n",
       " ('work', 0.8846593499183655)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Charan Reddy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('home', 0.752169132232666),\n",
       " ('him', 0.750031054019928),\n",
       " ('chance', 0.7305904626846313),\n",
       " ('away', 0.729337751865387),\n",
       " ('getting', 0.7244815826416016)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Charan Reddy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('into', 0.9998894929885864),\n",
       " ('even', 0.999889075756073),\n",
       " (\"'\", 0.9998788237571716),\n",
       " ('new', 0.9998748302459717),\n",
       " ('companies', 0.999873697757721)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What an astonishing thing a book is. It's a flat object made from a tree with flexible parts on which are imprinted lots of funny dark squiggles. But one glance at it and you're inside the mind of another person, maybe somebody dead for thousands of years. Across the millennia, an author is speaking clearly and silently inside your head, directly to you. Writing is perhaps the greatest of human inventions, binding together people who never knew each other, citizens of distant epochs. Books break the shackles of time. A book is proof that humans are capable of working magic.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('../Data/carl_sagan_quote.txt').read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    result = []\n",
    "    sent = sent_tokenize(text)\n",
    "    for sentence in sent:\n",
    "        words = word_tokenize(sentence)\n",
    "        tokens = [w for w in words if w.lower() not in string.punctuation]\n",
    "        stopw = stopwords.words('english')\n",
    "        tokens = [token for token in tokens if token not in stopw]\n",
    "    # remove words less than three letters\n",
    "        tokens = [word for word in tokens if len(word)>=3]\n",
    "    # lemmatize\n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokens = [lemma.lemmatize(word) for word in tokens]\n",
    "        result += [tokens] \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What', 'astonishing', 'thing', 'book'],\n",
       " ['flat',\n",
       "  'object',\n",
       "  'made',\n",
       "  'tree',\n",
       "  'flexible',\n",
       "  'part',\n",
       "  'imprinted',\n",
       "  'lot',\n",
       "  'funny',\n",
       "  'dark',\n",
       "  'squiggle'],\n",
       " ['But',\n",
       "  'one',\n",
       "  'glance',\n",
       "  \"'re\",\n",
       "  'inside',\n",
       "  'mind',\n",
       "  'another',\n",
       "  'person',\n",
       "  'maybe',\n",
       "  'somebody',\n",
       "  'dead',\n",
       "  'thousand',\n",
       "  'year'],\n",
       " ['Across',\n",
       "  'millennium',\n",
       "  'author',\n",
       "  'speaking',\n",
       "  'clearly',\n",
       "  'silently',\n",
       "  'inside',\n",
       "  'head',\n",
       "  'directly'],\n",
       " ['Writing',\n",
       "  'perhaps',\n",
       "  'greatest',\n",
       "  'human',\n",
       "  'invention',\n",
       "  'binding',\n",
       "  'together',\n",
       "  'people',\n",
       "  'never',\n",
       "  'knew',\n",
       "  'citizen',\n",
       "  'distant',\n",
       "  'epoch'],\n",
       " ['Books', 'break', 'shackle', 'time'],\n",
       " ['book', 'proof', 'human', 'capable', 'working', 'magic']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_p = preprocessing(text)\n",
    "text_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size – Denotes the number of dimensions present in the vectorial forms.\n",
    "# If you have read the document and have an idea of how many ‘topics’ it has, you can use that number\n",
    "# sg = 0 for CBOW model and 1 for skip-gram model\n",
    "# min_count: Ignore all words with total frequency lower than this\n",
    "# window: the maximum distance between the current and predicted word within\n",
    "# a sentence.\n",
    "model = Word2Vec(text_p, min_count=1, sg=1, window =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('together', 0.1858106106519699)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['millennium','human'], negative=['magic'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('together', 0.6569047570228577)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar_cosmul(positive=['millennium', 'human'], negative=['magic'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 0.17928019165992737)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['millennium','human','magic'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Charan Reddy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:858: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"millennium human magic book\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056537807"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('book', 'invention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word vectors are stored in a KeyedVectors instance in model.wv. \n",
    "#This separates the read-only word vector lookup operations in KeyedVectors from the training code in Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3431408e-04,  3.3194895e-03,  3.1520128e-03,  9.0304797e-04,\n",
       "        1.5009572e-03, -9.9173083e-04, -4.3051578e-03,  3.6845407e-03,\n",
       "       -3.9381688e-03,  3.4411668e-03,  4.5943572e-03,  4.8786676e-03,\n",
       "        1.2865782e-03,  2.0856906e-03, -5.8233639e-04, -3.5018048e-03,\n",
       "       -3.6876043e-03,  3.3343583e-03, -3.1211963e-03,  1.4298103e-03,\n",
       "       -3.6185309e-03,  4.3344432e-03,  4.6907997e-05, -3.6996172e-03,\n",
       "       -3.8957270e-03, -4.8625963e-03, -4.7180743e-04,  1.5920703e-03,\n",
       "       -2.7315671e-04, -2.9688335e-03, -4.4473894e-03, -2.1874483e-03,\n",
       "        1.3654934e-03, -4.4886568e-03, -8.1426190e-04,  1.4570822e-03,\n",
       "       -1.7163196e-03,  4.5871828e-03,  1.3340266e-03,  3.9657494e-03,\n",
       "       -2.0560875e-04,  1.4876358e-03,  2.5973532e-03,  2.5708866e-03,\n",
       "        1.3628926e-03,  3.4004492e-03, -3.9805779e-03, -1.8581826e-03,\n",
       "        4.7094389e-03, -1.1066807e-03, -4.1142269e-03, -4.3494683e-03,\n",
       "        3.1959724e-03,  1.4502425e-03, -1.3147068e-03,  1.7859532e-03,\n",
       "        4.6508485e-03, -4.2072316e-03,  1.1276556e-03,  1.3516780e-03,\n",
       "        1.0080653e-03,  2.0265169e-03,  4.7017583e-03,  1.6629439e-03,\n",
       "       -8.2301558e-04,  1.8215296e-03, -3.9133611e-03,  3.0500155e-03,\n",
       "        3.6868462e-03,  1.4805534e-03,  4.9953675e-03,  2.0354013e-03,\n",
       "       -3.3941232e-03, -3.7303043e-04,  3.8186777e-03,  4.8559071e-03,\n",
       "       -1.6763646e-03,  3.2182459e-03, -3.1042781e-03, -1.2188966e-03,\n",
       "       -8.1790728e-04, -4.8654028e-03,  1.9588163e-03, -6.9302187e-04,\n",
       "       -4.7746263e-03,  1.7461212e-03, -3.0831329e-03, -7.3850919e-05,\n",
       "       -3.8098334e-03, -3.4243846e-04, -8.1865065e-04, -2.3235946e-03,\n",
       "        2.3393952e-03,  4.1420395e-03,  7.2164665e-04,  4.7412035e-03,\n",
       "       -1.1614568e-03, -4.6067261e-03,  3.1331300e-03,  2.7968162e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word vector: vectorial representation.\n",
    "model.wv['book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'astonishing',\n",
       " 'thing',\n",
       " 'book',\n",
       " 'flat',\n",
       " 'object',\n",
       " 'made',\n",
       " 'tree',\n",
       " 'flexible',\n",
       " 'part']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(model.wv.vocab.keys())\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and Loading models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Data/word2vec_model')\n",
    "new_model = Word2Vec.load('../Data/word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glance', 0.1552678942680359),\n",
       " ('knew', 0.1399969458580017),\n",
       " ('dark', 0.12504993379116058)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.most_similar(negative=['human','magic'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('capable', 0.1726924180984497),\n",
       " ('perhaps', 0.17048539221286774),\n",
       " ('distant', 0.12994089722633362)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.most_similar(positive=['human','magic'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = Word2Vec(text_p, min_count=1, sg=1, window =3, hs=1, negative=0)\n",
    "model_.score([\"The cosmos a space time odyssey\".split()]) #Probability of a text under the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "Note: Don't worry about the warnings in gensim-FutureWarning. You can try with different versions of numpy to avoid them, but it's not a big deal.\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "https://pypi.python.org/pypi/gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
